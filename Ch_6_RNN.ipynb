{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Ethan Herron 3-4-2020\n",
    "\n",
    "This is my jupyter notebook for the book Deep Learning from Scratch by Seth Weidman. I will be following along with all of the code, and adding insights or questions I have along the way in these markdown cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "plt.style.use('seaborn-white')\n",
    "%matplotlib inline\n",
    "\n",
    "from copy import deepcopy\n",
    "from collections import deque\n",
    "\n",
    "from lincoln.utils.np_utils import assert_same_shape\n",
    "\n",
    "from scipy.special import logsumexp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: ndarray):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def dsigmoid(x: ndarray):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def tanh(x: ndarray):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def dtanh(x: ndarray):\n",
    "    return 1 - np.tanh(x) * np.tanh(x)\n",
    "\n",
    "def softmax(x, axis=None):\n",
    "    return np.exp(x - logsumexp(x, axis=axis, keepdims=True))\n",
    "\n",
    "def batch_softmax(input_array: ndarray):\n",
    "    out = []\n",
    "    for row in input_array:\n",
    "        out.append(softmax(row, axis=1))\n",
    "    return np.stack(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNOptimizer(object):\n",
    "    def __init__(self,\n",
    "                 lr: float = 0.01,\n",
    "                 gradient_clipping: bool = True) -> None:\n",
    "        self.lr = lr\n",
    "        self.gradient_clipping = gradient_clipping\n",
    "        self.first = True\n",
    "        \n",
    "    def step(self) -> None:\n",
    "        \n",
    "        for layer in self.model.layers:\n",
    "            for key in layer.params.keys():\n",
    "                \n",
    "                if self.gradient_clipping:\n",
    "                    np.clip(layer.params[key]['deriv'], -2, 2, layer.params[key]['deriv'])\n",
    "                    \n",
    "                self._update_rule(param=layer.params[key]['value'],\n",
    "                                  grad=layer.params[key]['deriv'])\n",
    "                \n",
    "    def _update_rule(self, **kwargs) -> None:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD and AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(RNNOptimizer):\n",
    "    def __init__(self,\n",
    "                 lr: float = 0.01,\n",
    "                 gradient_clipping: bool = True) -> None:\n",
    "        super().__init__(lr, gradient_clipping)\n",
    "        \n",
    "    def _update_rule(self, **kwargs) -> None:\n",
    "        \n",
    "        update = self.lr*kwargs['grad']\n",
    "        kwargs['param'] -= update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad(RNNOptimizer):\n",
    "    def __init__(self,\n",
    "                 lr: float = 0.01,\n",
    "                 gradient_clipping: bool = True) -> None:\n",
    "        super().__init__(lr, gradient_clipping)\n",
    "        self.eps = 1e-7\n",
    "        \n",
    "    def step(self) -> None:\n",
    "        if self.first:\n",
    "            self.sum_squares = {}\n",
    "            for i, layer in enumerate(self.model.layers):\n",
    "                self.sum_squares[i] = {}\n",
    "                for key in layer.params.keys():\n",
    "                    self.sum_squares[i][key] = np.zeros_like(layer.params[key]['value'])\n",
    "                    \n",
    "            self.first = False\n",
    "            \n",
    "        for i, layer in enumerate(self.model.layers):\n",
    "            for key in layer.params.keys():\n",
    "                \n",
    "                if self.gradient_clipping:\n",
    "                    np.clip(layer.params[key]['deriv'], -2, 2, layer.params[key]['deriv'])\n",
    "                \n",
    "                self._update_rule(param=layer.params[key]['value'],\n",
    "                                  grad=layer.params[key]['deriv'],\n",
    "                                  sum_square=self.sum_squares[i][key])\n",
    "                \n",
    "    def _update_rule(self, **kwargs) -> None:\n",
    "        \n",
    "        #update the running sum of squares\n",
    "        kwargs['sum_square'] += (self.eps +\n",
    "                                 np.power(kwargs['grad'], 2))\n",
    "        \n",
    "        #scale learning rate by running sum of squares = 5\n",
    "        lr = np.divide(self.lr, np.sqrt(kwargs['sum_square']))\n",
    "        \n",
    "        #use this to update parameters\n",
    "        kwargs['param'] -= lr * kwargs['grad']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self,\n",
    "                prediction: ndarray,\n",
    "                target: ndarray) -> float:\n",
    "        \n",
    "        assert_same_shape(prediction, target)\n",
    "        \n",
    "        self.prediction = prediction\n",
    "        self.target = target\n",
    "        \n",
    "        self.output = self._output()\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def backward(self) -> ndarray:\n",
    "        \n",
    "        self.input_grad = self._input_grad()\n",
    "        \n",
    "        assert_same_shape(self.prediction, self.input_grad)\n",
    "        \n",
    "        return self.input_grad\n",
    "    \n",
    "    def _output(self) -> float:\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def _input_grad(self) -> ndarray:\n",
    "        raise NotImpletmentedErrror()\n",
    "        \n",
    "class SoftmaxCrossEntropy(Loss):\n",
    "    \n",
    "    def __init__(self, eps: float=1e-9) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.single_class = False\n",
    "        \n",
    "    def _output(self) -> float:\n",
    "        \n",
    "        out = []\n",
    "        for row in self.prediction:\n",
    "            out.append(softmax(row, axis=1))\n",
    "        softmax_preds = np.stack(out)\n",
    "        \n",
    "        #clipping the softmax output to prevent numeric instability\n",
    "        self.softmax_preds = np.clip(softmax_preds, self.eps, 1 - self.eps)\n",
    "        \n",
    "        #actual loss computation similar to information theory p1 log(1-p1) idea\n",
    "        softmax_cross_entropy_loss = -1.0 * self.target * np.log(self.softmax_preds) - \\\n",
    "            (1.0 - self.target) * np.log(1 - self.softmax_preds)\n",
    "        \n",
    "        return np.sum(softmax_cross_entropy_loss)\n",
    "    \n",
    "    def _input_grad(self) -> ndarray:\n",
    "        return self.softmax_preds - self.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs\n",
    "## RNNNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNNode(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self,\n",
    "                x_in: ndarray,\n",
    "                H_in: ndarray,\n",
    "                params_dict: Dict[str, Dict[str, ndarray]]\n",
    "                ) -> Tuple[ndarray]:\n",
    "        '''\n",
    "        param x: np array of shape (batch_size, vocab_size)\n",
    "        param H_prev: np array of shape (batch_size, hidden_size)\n",
    "        return self.x_out: np array of shape (batch_size, vocab_size)\n",
    "        return self.H: np array of shape (batch_size, hidden_size)\n",
    "        '''\n",
    "        self.X_in = x_in\n",
    "        self.H_in = H_in\n",
    "        \n",
    "        self.Z = np.column_stack((x_in, H_in))\n",
    "        \n",
    "        self.H_int = np.dot(self.Z, params_dict['W_f']['value']) \\\n",
    "                                    + params_dict['B_f']['value']\n",
    "        \n",
    "        self.H_out = tanh(self.H_int)\n",
    "        \n",
    "        self.X_out = np.dot(self.H_out, params_dict['W_v']['value']) \\\n",
    "                                        + params_dict['B_v']['value']\n",
    "        \n",
    "        return self.X_out, self.H_out\n",
    "    \n",
    "    def backward(self,\n",
    "                 X_out_grad: ndarray,\n",
    "                 H_out_grad: ndarray,\n",
    "                 params_dict: Dict[str, Dict[str, ndarray]]) -> Tuple[ndarray]:\n",
    "        '''\n",
    "        param x_out_grad: np array of shape (batch_size, vocab_size)\n",
    "        param h_out_grad: np array shape (batch_size, hidden_size)\n",
    "        param RNN_params: RNN_params object\n",
    "        return x_in_grad: np array shape (batch_size, vocab_size)\n",
    "        return h_in_grad: np array shape (batch_size, hidden_size)\n",
    "        '''\n",
    "        \n",
    "        assert_same_shape(X_out_grad, self.X_out)\n",
    "        assert_same_shape(H_out_grad, self.H_out)\n",
    "        \n",
    "        params_dict['B_v']['deriv'] += X_out_grad.sum(axis=0)\n",
    "        params_dict['W_v']['deriv'] += np.dot(self.H_out.T, X_out_grad)\n",
    "        \n",
    "        dh = np.dot(X_out_grad, params_dict['W_v']['value'].T)\n",
    "        dh += H_out_grad\n",
    "        \n",
    "        dH_int = dh * dtanh(self.H_int)\n",
    "        \n",
    "        params_dict['B_f']['deriv'] += dH_int.sum(axis=0)\n",
    "        params_dict['W_f']['deriv'] += np.dot(self.Z.T, dH_int)\n",
    "        \n",
    "        dz = np.dot(dH_int, params_dict['W_f']['value'].T)\n",
    "        \n",
    "        X_in_grad = dz[:, :self.X_in.shape[1]]\n",
    "        H_in_grad = dz[:, self.X_in.shape[1]:]\n",
    "        \n",
    "        assert_same_shape(X_out_grad, self.X_out)\n",
    "        assert_same_shape(H_out_grad, self.H_out)\n",
    "        \n",
    "        return X_in_grad, H_in_grad        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLayer(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 hidden_size: int,\n",
    "                 output_size: int,\n",
    "                 weight_scale: float = None):\n",
    "        '''\n",
    "        param sequence_length: int - length of sequence being passed through the network\n",
    "        param vocab_size: int - the number of characters in the vocabulary of which we are predicting the next\n",
    "        character.\n",
    "        param hidden_size: int - the number of \"hidden neurons\" in the LSTM_Layer of which this node is a part.\n",
    "        param learning_rate: float - the learning rate\n",
    "        '''\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.weight_scale = weight_scale\n",
    "        self.start_H = np.zeros((1, hidden_size))\n",
    "        self.first = True\n",
    "\n",
    "    def _init_params(self,\n",
    "                     input_: ndarray):\n",
    "        \n",
    "        self.vocab_size = input_.shape[2]\n",
    "        \n",
    "        if not self.weight_scale:\n",
    "            self.weight_scale = 2 / (self.vocab_size + self.output_size)\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['W_f'] = {}\n",
    "        self.params['B_f'] = {}\n",
    "        self.params['W_v'] = {}\n",
    "        self.params['B_v'] = {}\n",
    "        \n",
    "        self.params['W_f']['value'] = np.random.normal(loc = 0.0,\n",
    "                                                      scale=self.weight_scale,\n",
    "                                                      size=(self.hidden_size + self.vocab_size, self.hidden_size))\n",
    "        self.params['B_f']['value'] = np.random.normal(loc = 0.0,\n",
    "                                                      scale=self.weight_scale,\n",
    "                                                      size=(1, self.hidden_size))\n",
    "        self.params['W_v']['value'] = np.random.normal(loc=0.0,\n",
    "                                                      scale=self.weight_scale,\n",
    "                                                      size=(self.hidden_size, self.output_size))\n",
    "        self.params['B_v']['value'] = np.random.normal(loc=0.0,\n",
    "                                                      scale=self.weight_scale,\n",
    "                                                      size=(1, self.output_size))    \n",
    "        \n",
    "        self.params['W_f']['deriv'] = np.zeros_like(self.params['W_f']['value'])\n",
    "        self.params['B_f']['deriv'] = np.zeros_like(self.params['B_f']['value'])\n",
    "        self.params['W_v']['deriv'] = np.zeros_like(self.params['W_v']['value'])\n",
    "        self.params['B_v']['deriv'] = np.zeros_like(self.params['B_v']['value'])\n",
    "        \n",
    "        self.cells = [RNNNode() for x in range(input_.shape[1])]\n",
    "\n",
    "    def _clear_gradients(self):\n",
    "        for key in self.params.keys():\n",
    "            self.params[key]['deriv'] = np.zeros_like(self.params[key]['deriv'])\n",
    "        \n",
    "\n",
    "    def forward(self, x_seq_in: ndarray):\n",
    "        '''\n",
    "        param x_seq_in: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
    "        return x_seq_out: numpy array of shape (batch_size, sequence_length, output_size)\n",
    "        '''\n",
    "        if self.first:\n",
    "            self._init_params(x_seq_in)\n",
    "            self.first=False\n",
    "        \n",
    "        batch_size = x_seq_in.shape[0]\n",
    "        \n",
    "        H_in = np.copy(self.start_H)\n",
    "        \n",
    "        H_in = np.repeat(H_in, batch_size, axis=0)\n",
    "\n",
    "        sequence_length = x_seq_in.shape[1]\n",
    "        \n",
    "        x_seq_out = np.zeros((batch_size, sequence_length, self.output_size))\n",
    "        \n",
    "        for t in range(sequence_length):\n",
    "\n",
    "            x_in = x_seq_in[:, t, :]\n",
    "            \n",
    "            y_out, H_in = self.cells[t].forward(x_in, H_in, self.params)\n",
    "      \n",
    "            x_seq_out[:, t, :] = y_out\n",
    "    \n",
    "        self.start_H = H_in.mean(axis=0, keepdims=True)\n",
    "        \n",
    "        return x_seq_out\n",
    "\n",
    "\n",
    "    def backward(self, x_seq_out_grad: ndarray):\n",
    "        '''\n",
    "        param loss_grad: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
    "        return loss_grad_out: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
    "        '''\n",
    "        batch_size = x_seq_out_grad.shape[0]\n",
    "        \n",
    "        h_in_grad = np.zeros((batch_size, self.hidden_size))\n",
    "        \n",
    "        sequence_length = x_seq_out_grad.shape[1]\n",
    "        \n",
    "        x_seq_in_grad = np.zeros((batch_size, sequence_length, self.vocab_size))\n",
    "        \n",
    "        for t in reversed(range(sequence_length)):\n",
    "            \n",
    "            x_out_grad = x_seq_out_grad[:, t, :]\n",
    "\n",
    "            grad_out, h_in_grad = \\\n",
    "                self.cells[t].backward(x_out_grad, h_in_grad, self.params)\n",
    "        \n",
    "            x_seq_in_grad[:, t, :] = grad_out\n",
    "        \n",
    "        return x_seq_in_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RNNModel(object):\n",
    "    '''\n",
    "    The Model class that takes in inputs and targets and actually trains the network and calculates the loss.\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 layers: List[RNNLayer],\n",
    "                 sequence_length: int, \n",
    "                 vocab_size: int, \n",
    "                 loss: Loss):\n",
    "        '''\n",
    "        param num_layers: int - the number of layers in the network\n",
    "        param sequence_length: int - length of sequence being passed through the network\n",
    "        param vocab_size: int - the number of characters in the vocabulary of which we are predicting the next\n",
    "        character.\n",
    "        param hidden_size: int - the number of \"hidden neurons\" in the each layer of the network.\n",
    "        '''\n",
    "        self.layers = layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.loss = loss\n",
    "        for layer in self.layers:\n",
    "            setattr(layer, 'sequence_length', sequence_length)\n",
    "\n",
    "        \n",
    "    def forward(self, \n",
    "                x_batch: ndarray):\n",
    "        '''\n",
    "        param inputs: list of integers - a list of indices of characters being passed in as the \n",
    "        input sequence of the network.\n",
    "        returns x_batch_in: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
    "        '''       \n",
    "        \n",
    "        for layer in self.layers:\n",
    "\n",
    "            x_batch = layer.forward(x_batch)\n",
    "                \n",
    "        return x_batch\n",
    "    \n",
    "    def backward(self, \n",
    "                 loss_grad: ndarray):\n",
    "        '''\n",
    "        param loss_grad: numpy array with shape (batch_size, sequence_length, vocab_size)\n",
    "        returns loss: float, representing mean squared error loss\n",
    "        '''\n",
    "\n",
    "        for layer in reversed(self.layers):\n",
    "\n",
    "            loss_grad = layer.backward(loss_grad)\n",
    "            \n",
    "        return loss_grad\n",
    "                \n",
    "    def single_step(self, \n",
    "                    x_batch: ndarray, \n",
    "                    y_batch: ndarray):\n",
    "        '''\n",
    "        The step that does it all:\n",
    "        1. Forward pass & softmax\n",
    "        2. Compute loss and loss gradient\n",
    "        3. Backward pass\n",
    "        4. Update parameters\n",
    "        param inputs: array of length sequence_length that represents the character indices of the inputs to\n",
    "        the network\n",
    "        param targets: array of length sequence_length that represents the character indices of the targets\n",
    "        of the network \n",
    "        return loss\n",
    "        '''  \n",
    "        \n",
    "        x_batch_out = self.forward(x_batch)\n",
    "        \n",
    "        loss = self.loss.forward(x_batch_out, y_batch)\n",
    "        \n",
    "        loss_grad = self.loss.backward()\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            layer._clear_gradients()\n",
    "        \n",
    "        self.backward(loss_grad)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNTrainer:\n",
    "    '''\n",
    "    Takes in a text file and a model, and starts generating characters.\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 text_file: str, \n",
    "                 model: RNNModel,\n",
    "                 optim: RNNOptimizer,\n",
    "                 batch_size: int = 32):\n",
    "        self.data = open(text_file, 'r').read()\n",
    "        self.model = model\n",
    "        self.chars = list(set(self.data))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.char_to_idx = {ch:i for i,ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i:ch for i,ch in enumerate(self.chars)}\n",
    "        self.sequence_length = self.model.sequence_length\n",
    "        self.batch_size = batch_size\n",
    "        self.optim = optim\n",
    "        setattr(self.optim, 'model', self.model)\n",
    "    \n",
    "\n",
    "    def _generate_inputs_targets(self, \n",
    "                                 start_pos: int):\n",
    "        \n",
    "        inputs_indices = np.zeros((self.batch_size, self.sequence_length), dtype=int)\n",
    "        targets_indices = np.zeros((self.batch_size, self.sequence_length), dtype=int)\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            \n",
    "            inputs_indices[i, :] = np.array([self.char_to_idx[ch] \n",
    "                            for ch in self.data[start_pos + i: start_pos + self.sequence_length  + i]])\n",
    "            targets_indices[i, :] = np.array([self.char_to_idx[ch] \n",
    "                         for ch in self.data[start_pos + 1 + i: start_pos + self.sequence_length + 1 + i]])\n",
    "\n",
    "        return inputs_indices, targets_indices\n",
    "\n",
    "\n",
    "    def _generate_one_hot_array(self, \n",
    "                                indices: ndarray):\n",
    "        '''\n",
    "        param indices: numpy array of shape (batch_size, sequence_length)\n",
    "        return batch - numpy array of shape (batch_size, sequence_length, vocab_size)\n",
    "        ''' \n",
    "        batch = []\n",
    "        for seq in indices:\n",
    "            \n",
    "            one_hot_sequence = np.zeros((self.sequence_length, self.vocab_size))\n",
    "            \n",
    "            for i in range(self.sequence_length):\n",
    "                one_hot_sequence[i, seq[i]] = 1.0\n",
    "\n",
    "            batch.append(one_hot_sequence) \n",
    "\n",
    "        return np.stack(batch)\n",
    "\n",
    "\n",
    "    def sample_output(self, \n",
    "                      input_char: int, \n",
    "                      sample_length: int):\n",
    "        '''\n",
    "        Generates a sample output using the current trained model, one character at a time.\n",
    "        param input_char: int - index of the character to use to start generating a sequence\n",
    "        param sample_length: int - the length of the sample output to generate\n",
    "        return txt: string - a string of length sample_length representing the sample output\n",
    "        '''\n",
    "        indices = []\n",
    "        \n",
    "        sample_model = deepcopy(self.model)\n",
    "        \n",
    "        for i in range(sample_length):\n",
    "            input_char_batch = np.zeros((1, 1, self.vocab_size))\n",
    "            \n",
    "            input_char_batch[0, 0, input_char] = 1.0\n",
    "            \n",
    "            x_batch_out = sample_model.forward(input_char_batch)\n",
    "            \n",
    "            x_softmax = batch_softmax(x_batch_out)\n",
    "            \n",
    "            input_char = np.random.choice(range(self.vocab_size), p=x_softmax.ravel())\n",
    "            \n",
    "            indices.append(input_char)\n",
    "            \n",
    "        txt = ''.join(self.idx_to_char[idx] for idx in indices)\n",
    "        return txt\n",
    "\n",
    "    def train(self, \n",
    "              num_iterations: int, \n",
    "              sample_every: int=100):\n",
    "        '''\n",
    "        Trains the \"character generator\" for a number of iterations. \n",
    "        Each \"iteration\" feeds a batch size of 1 through the neural network.\n",
    "        Continues until num_iterations is reached. Displays sample text generated using the latest version.\n",
    "        '''\n",
    "        plot_iter = np.zeros((0))\n",
    "        plot_loss = np.zeros((0))\n",
    "        \n",
    "        num_iter = 0\n",
    "        start_pos = 0\n",
    "        \n",
    "        moving_average = deque(maxlen=100)\n",
    "        while num_iter < num_iterations:\n",
    "            \n",
    "            if start_pos + self.sequence_length + self.batch_size + 1 > len(self.data):\n",
    "                start_pos = 0\n",
    "            \n",
    "            ## Update the model\n",
    "            inputs_indices, targets_indices = self._generate_inputs_targets(start_pos)\n",
    "\n",
    "            inputs_batch, targets_batch = \\\n",
    "                self._generate_one_hot_array(inputs_indices), self._generate_one_hot_array(targets_indices)\n",
    "            \n",
    "            loss = self.model.single_step(inputs_batch, targets_batch)\n",
    "            self.optim.step()\n",
    "            \n",
    "            moving_average.append(loss)\n",
    "            ma_loss = np.mean(moving_average)\n",
    "            \n",
    "            start_pos += self.batch_size\n",
    "            \n",
    "            plot_iter = np.append(plot_iter, [num_iter])\n",
    "            plot_loss = np.append(plot_loss, [ma_loss])\n",
    "            \n",
    "            if num_iter % 100 == 0:\n",
    "                plt.plot(plot_iter, plot_loss)\n",
    "                display.clear_output(wait=True)\n",
    "                plt.show()\n",
    "                \n",
    "                sample_text = self.sample_output(self.char_to_idx[self.data[start_pos]], \n",
    "                                                 200)\n",
    "                print(sample_text)\n",
    "\n",
    "            num_iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thed ak sour Tst Zhe ng\n",
      "ir soindd Cian:\n",
      "I bo shw\n",
      "Ohw, foups ange amm Fo, Ohan karhief\n",
      "BY;ikesae, po wl:\n",
      "I Guglit, an, tols and\n",
      "Mry,\n",
      "Wod aild kans wuny\n",
      "phtA the ple thous bathe cawC: yor yhinhi'lat Nim\n"
     ]
    }
   ],
   "source": [
    "layers = [RNNLayer(hidden_size=256, output_size=62)]\n",
    "mod = RNNModel(layers=layers,\n",
    "               vocab_size=62, sequence_length=10,\n",
    "               loss=SoftmaxCrossEntropy())\n",
    "optim = SGD(lr=0.001, gradient_clipping=True)\n",
    "trainer = RNNTrainer('input.txt', mod, optim)\n",
    "trainer.train(1000, sample_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So obviously this makes no sense. This is because the \"vanilla\" RNN is getting stuck in a local max.\n",
    "Now we will check performance with an LSTM\n",
    "# LSTMs\n",
    "## LSTMNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMNode:\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        param hidden_size: int - the number of \"hidden neurons\" in the LSTM_Layer of which this node is a part.\n",
    "        param vocab_size: int - the number of characters in the vocabulary of which we are predicting the next\n",
    "        character.\n",
    "        '''\n",
    "        pass\n",
    "        \n",
    "    def forward(self, \n",
    "                X_in: ndarray, \n",
    "                H_in: ndarray, \n",
    "                C_in: ndarray, \n",
    "                params_dict: Dict[str, Dict[str, ndarray]]):\n",
    "        '''\n",
    "        param X_in: numpy array of shape (batch_size, vocab_size)\n",
    "        param H_in: numpy array of shape (batch_size, hidden_size)\n",
    "        param C_in: numpy array of shape (batch_size, hidden_size)\n",
    "        return self.X_out: numpy array of shape (batch_size, output_size)\n",
    "        return self.H: numpy array of shape (batch_size, hidden_size)\n",
    "        return self.C: numpy array of shape (batch_size, hidden_size)\n",
    "        '''\n",
    "        self.X_in = X_in\n",
    "        self.C_in = C_in\n",
    "\n",
    "        self.Z = np.column_stack((X_in, H_in))\n",
    "        \n",
    "        self.f_int = np.dot(self.Z, params_dict['W_f']['value']) + params_dict['B_f']['value']\n",
    "        self.f = sigmoid(self.f_int)\n",
    "        \n",
    "        self.i_int = np.dot(self.Z, params_dict['W_i']['value']) + params_dict['B_i']['value']\n",
    "        self.i = sigmoid(self.i_int)\n",
    "        self.C_bar_int = np.dot(self.Z, params_dict['W_c']['value']) + params_dict['B_c']['value']\n",
    "        self.C_bar = tanh(self.C_bar_int)\n",
    "\n",
    "        self.C_out = self.f * C_in + self.i * self.C_bar\n",
    "        self.o_int = np.dot(self.Z, params_dict['W_o']['value']) + params_dict['B_o']['value']\n",
    "        self.o = sigmoid(self.o_int)\n",
    "        self.H_out = self.o * tanh(self.C_out)\n",
    "\n",
    "        self.X_out = np.dot(self.H_out, params_dict['W_v']['value']) + params_dict['B_v']['value']\n",
    "        \n",
    "        return self.X_out, self.H_out, self.C_out \n",
    "\n",
    "\n",
    "    def backward(self, \n",
    "                 X_out_grad: ndarray, \n",
    "                 H_out_grad: ndarray, \n",
    "                 C_out_grad: ndarray, \n",
    "                 params_dict: Dict[str, Dict[str, ndarray]]):\n",
    "        '''\n",
    "        param loss_grad: numpy array of shape (1, vocab_size)\n",
    "        param dh_next: numpy array of shape (1, hidden_size)\n",
    "        param dC_next: numpy array of shape (1, hidden_size)\n",
    "        param LSTM_Params: LSTM_Params object\n",
    "        return self.dx_prev: numpy array of shape (1, vocab_size)\n",
    "        return self.dH_prev: numpy array of shape (1, hidden_size)\n",
    "        return self.dC_prev: numpy array of shape (1, hidden_size)\n",
    "        '''\n",
    "        \n",
    "        assert_same_shape(X_out_grad, self.X_out)\n",
    "        assert_same_shape(H_out_grad, self.H_out)\n",
    "        assert_same_shape(C_out_grad, self.C_out)\n",
    "\n",
    "        params_dict['W_v']['deriv'] += np.dot(self.H_out.T, X_out_grad)\n",
    "        params_dict['B_v']['deriv'] += X_out_grad.sum(axis=0)\n",
    "\n",
    "        dh_out = np.dot(X_out_grad, params_dict['W_v']['value'].T)        \n",
    "        dh_out += H_out_grad\n",
    "                         \n",
    "        do = dh_out * tanh(self.C_out)\n",
    "        do_int = dsigmoid(self.o_int) * do\n",
    "        params_dict['W_o']['deriv'] += np.dot(self.Z.T, do_int)\n",
    "        params_dict['B_o']['deriv'] += do_int.sum(axis=0)\n",
    "\n",
    "        dC_out = dh_out * self.o * dtanh(self.C_out)\n",
    "        dC_out += C_out_grad\n",
    "        dC_bar = dC_out * self.i\n",
    "        dC_bar_int = dtanh(self.C_bar_int) * dC_bar\n",
    "        params_dict['W_c']['deriv'] += np.dot(self.Z.T, dC_bar_int)\n",
    "        params_dict['B_c']['deriv'] += dC_bar_int.sum(axis=0)\n",
    "\n",
    "        di = dC_out * self.C_bar\n",
    "        di_int = dsigmoid(self.i_int) * di\n",
    "        params_dict['W_i']['deriv'] += np.dot(self.Z.T, di_int)\n",
    "        params_dict['B_i']['deriv'] += di_int.sum(axis=0)\n",
    "\n",
    "        df = dC_out * self.C_in\n",
    "        df_int = dsigmoid(self.f_int) * df\n",
    "        params_dict['W_f']['deriv'] += np.dot(self.Z.T, df_int)\n",
    "        params_dict['B_f']['deriv'] += df_int.sum(axis=0)\n",
    "\n",
    "        dz = (np.dot(df_int, params_dict['W_f']['value'].T)\n",
    "             + np.dot(di_int, params_dict['W_i']['value'].T)\n",
    "             + np.dot(dC_bar_int, params_dict['W_c']['value'].T)\n",
    "             + np.dot(do_int, params_dict['W_o']['value'].T))\n",
    "    \n",
    "        dx_prev = dz[:, :self.X_in.shape[1]]\n",
    "        dH_prev = dz[:, self.X_in.shape[1]:]\n",
    "        dC_prev = self.f * dC_out\n",
    "\n",
    "        return dx_prev, dH_prev, dC_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayer:\n",
    "\n",
    "    def __init__(self,\n",
    "                 hidden_size: int,\n",
    "                 output_size: int,\n",
    "                 weight_scale: float = 0.01):\n",
    "        '''\n",
    "        param sequence_length: int - length of sequence being passed through the network\n",
    "        param vocab_size: int - the number of characters in the vocabulary of which we are predicting the next\n",
    "        character.\n",
    "        param hidden_size: int - the number of \"hidden neurons\" in the LSTM_Layer of which this node is a part.\n",
    "        param learning_rate: float - the learning rate\n",
    "        '''\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.weight_scale = weight_scale\n",
    "        self.start_H = np.zeros((1, hidden_size))\n",
    "        self.start_C = np.zeros((1, hidden_size))        \n",
    "        self.first = True\n",
    "\n",
    "        \n",
    "    def _init_params(self,\n",
    "                     input_: ndarray):\n",
    "        \n",
    "        self.vocab_size = input_.shape[2]\n",
    "\n",
    "        self.params = {}\n",
    "        self.params['W_f'] = {}\n",
    "        self.params['B_f'] = {}\n",
    "        self.params['W_i'] = {}\n",
    "        self.params['B_i'] = {}\n",
    "        self.params['W_c'] = {}\n",
    "        self.params['B_c'] = {}\n",
    "        self.params['W_o'] = {}\n",
    "        self.params['B_o'] = {}        \n",
    "        self.params['W_v'] = {}\n",
    "        self.params['B_v'] = {}\n",
    "        \n",
    "        self.params['W_f']['value'] = np.random.normal(loc=0.0,\n",
    "                                                       scale=self.weight_scale,\n",
    "                                                       size =(self.hidden_size + self.vocab_size, self.hidden_size))\n",
    "        self.params['B_f']['value'] = np.random.normal(loc=0.0,\n",
    "                                                       scale=self.weight_scale,\n",
    "                                                       size=(1, self.hidden_size))\n",
    "        self.params['W_i']['value'] = np.random.normal(loc=0.0,\n",
    "                                                       scale=self.weight_scale,\n",
    "                                                       size=(self.hidden_size + self.vocab_size, self.hidden_size))\n",
    "        self.params['B_i']['value'] = np.random.normal(loc=0.0,\n",
    "                                                      scale=self.weight_scale,\n",
    "                                                      size=(1, self.hidden_size))\n",
    "        self.params['W_c']['value'] = np.random.normal(loc=0.0,\n",
    "                                                      scale=self.weight_scale,\n",
    "                                                      size=(self.hidden_size + self.vocab_size, self.hidden_size))\n",
    "        self.params['B_c']['value'] = np.random.normal(loc=0.0,\n",
    "                                                      scale=self.weight_scale,\n",
    "                                                      size=(1, self.hidden_size))\n",
    "        self.params['W_o']['value'] = np.random.normal(loc=0.0,\n",
    "                                                      scale=self.weight_scale,\n",
    "                                                      size=(self.hidden_size + self.vocab_size, self.hidden_size))\n",
    "        self.params['B_o']['value'] = np.random.normal(loc=0.0,\n",
    "                                                      scale=self.weight_scale,\n",
    "                                                      size=(1, self.hidden_size))       \n",
    "        self.params['W_v']['value'] = np.random.normal(loc=0.0,\n",
    "                                                      scale=self.weight_scale,\n",
    "                                                      size=(self.hidden_size, self.output_size))\n",
    "        self.params['B_v']['value'] = np.random.normal(loc=0.0,\n",
    "                                                      scale=self.weight_scale,\n",
    "                                                      size=(1, self.output_size))\n",
    "        \n",
    "        for key in self.params.keys():\n",
    "            self.params[key]['deriv'] = np.zeros_like(self.params[key]['value'])\n",
    "        \n",
    "        self.cells = [LSTMNode() for x in range(input_.shape[1])]\n",
    "\n",
    "\n",
    "    def _clear_gradients(self):\n",
    "        for key in self.params.keys():\n",
    "            self.params[key]['deriv'] = np.zeros_like(self.params[key]['deriv'])\n",
    "                    \n",
    "        \n",
    "    def forward(self, x_seq_in: ndarray):\n",
    "        '''\n",
    "        param x_seq_in: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
    "        return x_seq_out: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
    "        '''\n",
    "        if self.first:\n",
    "            self._init_params(x_seq_in)\n",
    "            self.first=False\n",
    "        \n",
    "        batch_size = x_seq_in.shape[0]\n",
    "        \n",
    "        H_in = np.copy(self.start_H)\n",
    "        C_in = np.copy(self.start_C)\n",
    "        \n",
    "        H_in = np.repeat(H_in, batch_size, axis=0)\n",
    "        C_in = np.repeat(C_in, batch_size, axis=0)        \n",
    "\n",
    "        sequence_length = x_seq_in.shape[1]\n",
    "        \n",
    "        x_seq_out = np.zeros((batch_size, sequence_length, self.output_size))\n",
    "        \n",
    "        for t in range(sequence_length):\n",
    "\n",
    "            x_in = x_seq_in[:, t, :]\n",
    "            \n",
    "            y_out, H_in, C_in = self.cells[t].forward(x_in, H_in, C_in, self.params)\n",
    "      \n",
    "            x_seq_out[:, t, :] = y_out\n",
    "    \n",
    "        self.start_H = H_in.mean(axis=0, keepdims=True)\n",
    "        self.start_C = C_in.mean(axis=0, keepdims=True)        \n",
    "        \n",
    "        return x_seq_out\n",
    "\n",
    "\n",
    "    def backward(self, x_seq_out_grad: ndarray):\n",
    "        '''\n",
    "        param loss_grad: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
    "        return loss_grad_out: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
    "        '''\n",
    "        \n",
    "        batch_size = x_seq_out_grad.shape[0]\n",
    "        \n",
    "        h_in_grad = np.zeros((batch_size, self.hidden_size))\n",
    "        c_in_grad = np.zeros((batch_size, self.hidden_size))        \n",
    "        \n",
    "        num_chars = x_seq_out_grad.shape[1]\n",
    "        \n",
    "        x_seq_in_grad = np.zeros((batch_size, num_chars, self.vocab_size))\n",
    "        \n",
    "        for t in reversed(range(num_chars)):\n",
    "            \n",
    "            x_out_grad = x_seq_out_grad[:, t, :]\n",
    "\n",
    "            grad_out, h_in_grad, c_in_grad = \\\n",
    "                self.cells[t].backward(x_out_grad, h_in_grad, c_in_grad, self.params)\n",
    "        \n",
    "            x_seq_in_grad[:, t, :] = grad_out\n",
    "        \n",
    "        return x_seq_in_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(object):\n",
    "    '''\n",
    "    The Model class that takes in inputs and targets and actually trains the network and calculates the loss.\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 layers: List[LSTMLayer],\n",
    "                 sequence_length: int, \n",
    "                 vocab_size: int, \n",
    "                 hidden_size: int,\n",
    "                 loss: Loss):\n",
    "        '''\n",
    "        param num_layers: int - the number of layers in the network\n",
    "        param sequence_length: int - length of sequence being passed through the network\n",
    "        param vocab_size: int - the number of characters in the vocabulary of which we are predicting the next\n",
    "        character.\n",
    "        param hidden_size: int - the number of \"hidden neurons\" in the each layer of the network.\n",
    "        '''\n",
    "        self.layers = layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.loss = loss\n",
    "        for layer in self.layers:\n",
    "            setattr(layer, 'sequence_length', sequence_length)\n",
    "\n",
    "        \n",
    "    def forward(self, \n",
    "                x_batch: ndarray):\n",
    "        '''\n",
    "        param inputs: list of integers - a list of indices of characters being passed in as the \n",
    "        input sequence of the network.\n",
    "        returns x_batch_in: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
    "        '''       \n",
    "        \n",
    "        for layer in self.layers:\n",
    "\n",
    "            x_batch = layer.forward(x_batch)\n",
    "                \n",
    "        return x_batch\n",
    "        \n",
    "    def backward(self, \n",
    "                 loss_grad: ndarray):\n",
    "        '''\n",
    "        param loss_grad: numpy array with shape (batch_size, sequence_length, vocab_size)\n",
    "        returns loss: float, representing mean squared error loss\n",
    "        '''\n",
    "\n",
    "        for layer in reversed(self.layers):\n",
    "\n",
    "            loss_grad = layer.backward(loss_grad)\n",
    "            \n",
    "        return loss_grad\n",
    "                \n",
    "    def single_step(self, \n",
    "                    x_batch: ndarray, \n",
    "                    y_batch: ndarray):\n",
    "        '''\n",
    "        The step that does it all:\n",
    "        1. Forward pass & softmax\n",
    "        2. Compute loss and loss gradient\n",
    "        3. Backward pass\n",
    "        4. Update parameters\n",
    "        param inputs: array of length sequence_length that represents the character indices of the inputs to\n",
    "        the network\n",
    "        param targets: array of length sequence_length that represents the character indices of the targets\n",
    "        of the network \n",
    "        return loss\n",
    "        '''  \n",
    "        \n",
    "        x_batch_out = self.forward(x_batch)\n",
    "        \n",
    "        loss = self.loss.forward(x_batch_out, y_batch)\n",
    "        \n",
    "        loss_grad = self.loss.backward()\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            layer._clear_gradients()\n",
    "        \n",
    "        self.backward(loss_grad)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU\n",
    "## GRUNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUNode(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        param hidden_size: int - the number of \"hidden neurons\" in the LSTM_Layer of which this node is a part.\n",
    "        param vocab_size: int - the number of characters in the vocabulary of which we are predicting the next\n",
    "        character.\n",
    "        '''\n",
    "        pass\n",
    "        \n",
    "    def forward(self, \n",
    "                X_in: ndarray, \n",
    "                H_in: ndarray,\n",
    "                params_dict: Dict[str, Dict[str, ndarray]]) -> Tuple[ndarray]:\n",
    "        '''\n",
    "        param X_in: numpy array of shape (batch_size, vocab_size)\n",
    "        param H_in: numpy array of shape (batch_size, hidden_size)\n",
    "        return self.X_out: numpy array of shape (batch_size, vocab_size)\n",
    "        return self.H_out: numpy array of shape (batch_size, hidden_size)\n",
    "        '''\n",
    "        self.X_in = X_in\n",
    "        self.H_in = H_in        \n",
    "        \n",
    "        # reset gate\n",
    "        self.X_r = np.dot(X_in, params_dict['W_xr']['value'])\n",
    "        self.H_r = np.dot(H_in, params_dict['W_hr']['value'])\n",
    "\n",
    "        # update gate        \n",
    "        self.X_u = np.dot(X_in, params_dict['W_xu']['value'])\n",
    "        self.H_u = np.dot(H_in, params_dict['W_hu']['value'])        \n",
    "        \n",
    "        # gates   \n",
    "        self.r_int = self.X_r + self.H_r + params_dict['B_r']['value']\n",
    "        self.r = sigmoid(self.r_int)\n",
    "        \n",
    "        self.u_int = self.X_r + self.H_r + params_dict['B_u']['value']\n",
    "        self.u = sigmoid(self.u_int)\n",
    "\n",
    "        # new state        \n",
    "        self.h_reset = self.r * H_in\n",
    "        self.X_h = np.dot(X_in, params_dict['W_xh']['value'])\n",
    "        self.H_h = np.dot(self.h_reset, params_dict['W_hh']['value']) \n",
    "        self.h_bar_int = self.X_h + self.H_h + params_dict['B_h']['value']\n",
    "        self.h_bar = tanh(self.h_bar_int)        \n",
    "        \n",
    "        self.H_out = self.u * self.H_in + (1 - self.u) * self.h_bar\n",
    "\n",
    "        self.X_out = np.dot(self.H_out, params_dict['W_v']['value']) + params_dict['B_v']['value']\n",
    "        \n",
    "        return self.X_out, self.H_out\n",
    "\n",
    "\n",
    "    def backward(self, \n",
    "                 X_out_grad: ndarray, \n",
    "                 H_out_grad: ndarray, \n",
    "                 params_dict: Dict[str, Dict[str, ndarray]]):\n",
    "        \n",
    "        params_dict['B_v']['deriv'] += X_out_grad.sum(axis=0)\n",
    "        params_dict['W_v']['deriv'] += np.dot(self.H_out.T, X_out_grad)\n",
    "\n",
    "        dh_out = np.dot(X_out_grad, params_dict['W_v']['value'].T)        \n",
    "        dh_out += H_out_grad\n",
    "                         \n",
    "        du = self.H_in * H_out_grad - self.h_bar * H_out_grad \n",
    "        dh_bar = (1 - self.u) * H_out_grad\n",
    "        \n",
    "        dh_bar_int = dh_bar * dtanh(self.h_bar_int)\n",
    "        params_dict['B_h']['deriv'] += dh_bar_int.sum(axis=0)\n",
    "        params_dict['W_xh']['deriv'] += np.dot(self.X_in.T, dh_bar_int)\n",
    "        \n",
    "        dX_in = np.dot(dh_bar_int, params_dict['W_xh']['value'].T)\n",
    " \n",
    "        params_dict['W_hh']['deriv'] += np.dot(self.h_reset.T, dh_bar_int)\n",
    "        dh_reset = np.dot(dh_bar_int, params_dict['W_hh']['value'].T)   \n",
    "        \n",
    "        dr = dh_reset * self.H_in\n",
    "        dH_in = dh_reset * self.r        \n",
    "        \n",
    "        # update branch\n",
    "        du_int = dsigmoid(self.u_int) * du\n",
    "        params_dict['B_u']['deriv'] += du_int.sum(axis=0)\n",
    "\n",
    "        dX_in += np.dot(du_int, params_dict['W_xu']['value'].T)\n",
    "        params_dict['W_xu']['deriv'] += np.dot(self.X_in.T, du_int)\n",
    "        \n",
    "        dH_in += np.dot(du_int, params_dict['W_hu']['value'].T)\n",
    "        params_dict['W_hu']['deriv'] += np.dot(self.H_in.T, du_int)        \n",
    "\n",
    "        # reset branch\n",
    "        dr_int = dsigmoid(self.r_int) * dr\n",
    "        params_dict['B_r']['deriv'] += dr_int.sum(axis=0)\n",
    "\n",
    "        dX_in += np.dot(dr_int, params_dict['W_xr']['value'].T)\n",
    "        params_dict['W_xr']['deriv'] += np.dot(self.X_in.T, dr_int)\n",
    "        \n",
    "        dH_in += np.dot(dr_int, params_dict['W_hr']['value'].T)\n",
    "        params_dict['W_hr']['deriv'] += np.dot(self.H_in.T, dr_int)   \n",
    "        \n",
    "        return dX_in, dH_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRULayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRULayer(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 hidden_size: int,\n",
    "                 output_size: int,\n",
    "                 weight_scale: float = 0.01):\n",
    "        '''\n",
    "        param sequence_length: int - length of sequence being passed through the network\n",
    "        param vocab_size: int - the number of characters in the vocabulary of which we are predicting the next\n",
    "        character.\n",
    "        param hidden_size: int - the number of \"hidden neurons\" in the LSTM_Layer of which this node is a part.\n",
    "        param learning_rate: float - the learning rate\n",
    "        '''\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.weight_scale = weight_scale\n",
    "        self.start_H = np.zeros((1, hidden_size))       \n",
    "        self.first = True\n",
    "\n",
    "        \n",
    "    def _init_params(self,\n",
    "                     input_: ndarray):\n",
    "        \n",
    "        self.vocab_size = input_.shape[2]\n",
    "\n",
    "        self.params = {}\n",
    "        self.params['W_xr'] = {}\n",
    "        self.params['W_hr'] = {}\n",
    "        self.params['B_r'] = {}\n",
    "        self.params['W_xu'] = {}\n",
    "        self.params['W_hu'] = {}\n",
    "        self.params['B_u'] = {}\n",
    "        self.params['W_xh'] = {}\n",
    "        self.params['W_hh'] = {}\n",
    "        self.params['B_h'] = {}        \n",
    "        self.params['W_v'] = {}\n",
    "        self.params['B_v'] = {}\n",
    "        \n",
    "        self.params['W_xr']['value'] = np.random.normal(loc=0.0,\n",
    "                                                        scale=self.weight_scale,\n",
    "                                                        size=(self.vocab_size, self.hidden_size))\n",
    "        self.params['W_hr']['value'] = np.random.normal(loc=0.0,\n",
    "                                                        scale=self.weight_scale,\n",
    "                                                        size=(self.hidden_size, self.hidden_size))        \n",
    "        self.params['B_r']['value'] = np.random.normal(loc=0.0,\n",
    "                                                       scale=self.weight_scale,\n",
    "                                                       size=(1, self.hidden_size))\n",
    "        self.params['W_xu']['value'] = np.random.normal(loc=0.0,\n",
    "                                                        scale=self.weight_scale,\n",
    "                                                        size=(self.vocab_size, self.hidden_size))\n",
    "        self.params['W_hu']['value'] = np.random.normal(loc=0.0,\n",
    "                                                       scale=self.weight_scale,\n",
    "                                                       size=(self.hidden_size, self.hidden_size))\n",
    "        self.params['B_u']['value'] = np.random.normal(loc=0.0,\n",
    "                                                      scale=self.weight_scale,\n",
    "                                                      size=(1, self.hidden_size))\n",
    "        self.params['W_xh']['value'] = np.random.normal(loc=0.0,\n",
    "                                                       scale=self.weight_scale,\n",
    "                                                       size=(self.vocab_size, self.hidden_size))\n",
    "        self.params['W_hh']['value'] = np.random.normal(loc=0.0,\n",
    "                                                       scale=self.weight_scale,\n",
    "                                                       size=(self.hidden_size, self.hidden_size))\n",
    "        self.params['B_h']['value'] = np.random.normal(loc=0.0,\n",
    "                                                       scale=1.0,\n",
    "                                                       size=(1, self.hidden_size))\n",
    "        self.params['W_v']['value'] = np.random.normal(loc=0.0,\n",
    "                                                       scale=1.0,\n",
    "                                                       size=(self.hidden_size, self.output_size))\n",
    "        self.params['B_v']['value'] = np.random.normal(loc=0.0,\n",
    "                                                       scale=1.0,\n",
    "                                                       size=(1, self.output_size))    \n",
    "        \n",
    "        for key in self.params.keys():\n",
    "            self.params[key]['deriv'] = np.zeros_like(self.params[key]['value'])\n",
    "        \n",
    "        self.cells = [GRUNode() for x in range(input_.shape[1])]\n",
    "\n",
    "\n",
    "    def _clear_gradients(self):\n",
    "        for key in self.params.keys():\n",
    "            self.params[key]['deriv'] = np.zeros_like(self.params[key]['deriv'])\n",
    "                    \n",
    "        \n",
    "    def forward(self, x_seq_in: ndarray):\n",
    "        '''\n",
    "        param x_seq_in: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
    "        return x_seq_out: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
    "        '''\n",
    "        if self.first:\n",
    "            self._init_params(x_seq_in)\n",
    "            self.first=False\n",
    "        \n",
    "        batch_size = x_seq_in.shape[0]\n",
    "        \n",
    "        H_in = np.copy(self.start_H)\n",
    "\n",
    "        H_in = np.repeat(H_in, batch_size, axis=0)      \n",
    "\n",
    "        sequence_length = x_seq_in.shape[1]\n",
    "        \n",
    "        x_seq_out = np.zeros((batch_size, sequence_length, self.output_size))\n",
    "        \n",
    "        for t in range(sequence_length):\n",
    "\n",
    "            x_in = x_seq_in[:, t, :]\n",
    "            \n",
    "            y_out, H_in = self.cells[t].forward(x_in, H_in, self.params)\n",
    "      \n",
    "            x_seq_out[:, t, :] = y_out\n",
    "    \n",
    "        self.start_H = H_in.mean(axis=0, keepdims=True)     \n",
    "        \n",
    "        return x_seq_out\n",
    "\n",
    "\n",
    "    def backward(self, x_seq_out_grad: ndarray):\n",
    "        '''\n",
    "        param loss_grad: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
    "        return loss_grad_out: numpy array of shape (batch_size, sequence_length, vocab_size)\n",
    "        '''\n",
    "        \n",
    "        batch_size = x_seq_out_grad.shape[0]\n",
    "        \n",
    "        h_in_grad = np.zeros((batch_size, self.hidden_size))        \n",
    "        \n",
    "        num_chars = x_seq_out_grad.shape[1]\n",
    "        \n",
    "        x_seq_in_grad = np.zeros((batch_size, num_chars, self.vocab_size))\n",
    "        \n",
    "        for t in reversed(range(num_chars)):\n",
    "            \n",
    "            x_out_grad = x_seq_out_grad[:, t, :]\n",
    "\n",
    "            grad_out, h_in_grad = \\\n",
    "                self.cells[t].backward(x_out_grad, h_in_grad, self.params)\n",
    "        \n",
    "            x_seq_in_grad[:, t, :] = grad_out\n",
    "        \n",
    "        return x_seq_in_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "## Single LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nraWesr\n",
      "\n",
      "Iag:\n",
      ":nwlsd n aea awehr\n",
      "\n",
      "nopyNdOs,: haBNrnccftn U\n",
      "Hm Kw hrn.sonisd aTet Nieect\n",
      "Bap cernNraot nt nn\n",
      "he\n",
      "iGepn se :e\n",
      "he\n",
      "NW:t Ranecc esh of  s crnmidco\n",
      "?\n",
      "DYoHFBt cs n\n",
      "thphataWrnd\n",
      "B?R Tgt inoe ry\n",
      "\n"
     ]
    }
   ],
   "source": [
    "layers1 = [LSTMLayer(hidden_size=256, output_size=62, weight_scale=0.01)]\n",
    "mod = RNNModel(layers=layers1,\n",
    "               vocab_size=62, sequence_length=25,\n",
    "               loss=SoftmaxCrossEntropy())\n",
    "optim = AdaGrad(lr=0.01, gradient_clipping=True)\n",
    "trainer = RNNTrainer('input.txt', mod, optim, batch_size=3)\n",
    "trainer.train(1000, sample_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three different variations of multiple layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ";\n",
      "It\n",
      "OLTNCIAMA OORNIIvRDL:\n",
      "Bhe o ots as yuul ist RIcSOLEO:\n",
      "Oe \n",
      "\n",
      "Fo!kL\n",
      "OE?\n",
      "PON'QOIWLTTAIC:\n",
      "vove, ansase is, , densd, halasH I Fhanew\n",
      "hafh :rb,\n",
      "\n",
      "TRI:NLAA A Tnm:\n",
      "B KlSONB:\n",
      "Shio a band, Put\n",
      "ITEAIoNTL:\n",
      "Doy\n"
     ]
    }
   ],
   "source": [
    "layers2 = [RNNLayer(hidden_size=256, output_size=128, weight_scale=0.1),\n",
    "           LSTMLayer(hidden_size=256, output_size=62, weight_scale=0.01)]\n",
    "mod = RNNModel(layers=layers2,\n",
    "               vocab_size=62, sequence_length=25,\n",
    "               loss=SoftmaxCrossEntropy())\n",
    "optim = AdaGrad(lr=0.01, gradient_clipping=True)\n",
    "trainer = RNNTrainer('input.txt', mod, optim, batch_size=32)\n",
    "trainer.train(2000, sample_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers2 = [LSTMLayer(hidden_size=256, output_size=128, weight_scale=0.1),\n",
    "           LSTMLayer(hidden_size=256, output_size=62, weight_scale=0.01)]\n",
    "mod = RNNModel(layers=layers2,\n",
    "               vocab_size=62, sequence_length=25,\n",
    "               loss=SoftmaxCrossEntropy())\n",
    "optim = SGD(lr=0.01, gradient_clipping=True)\n",
    "trainer = RNNTrainer('input.txt', mod, optim, batch_size=32)\n",
    "trainer.train(2000, sample_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers3 = [GRULayer(hidden_size=256, output_size=128, weight_scale=0.1),\n",
    "           LSTMLayer(hidden_size=256, output_size=62, weight_scale=0.01)]\n",
    "mod = RNNModel(layers=layers3,\n",
    "               vocab_size=62, sequence_length=25,\n",
    "               loss=SoftmaxCrossEntropy())\n",
    "optim = AdaGrad(lr=0.01, gradient_clipping=True)\n",
    "trainer = RNNTrainer('input.txt', mod, optim, batch_size=32)\n",
    "trainer.train(2000, sample_every=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
